{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Overview of the Bert + BI-LSTM Classification Model\n","\n","The `Bert + BI-LSTM Classification` model combines the strengths of BERT for deep contextual embedding with the sequential processing capabilities of a bidirectional LSTM (Long Short-Term Memory) network. This hybrid approach aims to leverage the contextual insights provided by BERT along with the LSTM's ability to capture dependencies in sequences over long distances, making it particularly suited for tasks like sentiment analysis or contextual classification where understanding the entire sequence is crucial.\n","\n","### Key Components of the Model:\n","\n","1. **BERT Model**:\n","   - Utilized as the initial embedding layer to convert input text tokens into rich, contextualized embeddings. BERT's pre-trained models are adept at understanding complex language nuances, which forms a robust foundation for further sequence processing.\n","\n","2. **Bidirectional LSTM**:\n","   - Follows the BERT embedding layer, allowing the model to process text embeddings in both forward and backward directions across the text sequence. This bidirectionality helps the model to capture context from both past and future tokens simultaneously, enhancing its ability to understand the overall sequence.\n","\n","3. **Dropout and Linear Layer**:\n","   - A dropout layer is applied after the LSTM to prevent overfitting by randomly zeroing out some of the features. This is followed by a linear layer that maps the LSTM outputs to the final classification labels.\n","\n","### Model Architecture Details:\n","\n","- **Input**: The model takes tokenized text input, which is processed by BERT to produce embeddings. These embeddings are then fed into a bidirectional LSTM.\n","- **Output**: The final output is generated through a linear layer that classifies the text into predefined categories based on the learned features.\n","\n","### Implementation Details:\n","\n","1. **Data Preprocessing and Tokenization**:\n","   - Text data is cleaned to remove URLs, user mentions, numerical data, and special characters to standardize the input and focus on meaningful text content.\n","   - The BERT tokenizer converts cleaned text into tokens that are suitable for processing by the BERT model.\n","\n","2. **Dataset and DataLoader**:\n","   - A custom `TweetDataset` class manages the tokenized data and labels, ensuring efficient batch processing during training and evaluation via PyTorch’s `DataLoader`.\n","\n","3. **Model Training and Evaluation**:\n","   - Training involves multiple epochs where the model learns by minimizing the cross-entropy loss between predicted and actual labels. AdamW optimizer is used for effective weight updates.\n","   - Evaluation assesses model performance on validation and test datasets to ensure generalizability beyond the training data.\n","\n","4. **Computational Efficiency**:\n","   - The model is designed to run on GPU if available, significantly speeding up computations necessary for training and inference phases.\n","\n","5. **Loss and Accuracy Computation**:\n","   - During training and validation, loss and accuracy metrics are computed to monitor the model's performance and guide training decisions.\n","\n","### Usage and Application:\n","\n","This hybrid model is particularly effective for nuanced text classification tasks where both the context provided by individual words and the overall sequence of words play crucial roles. Examples include sentiment analysis, topic classification, and other NLP tasks requiring a deep understanding of language context.\n","\n","### Benefits of the BertLSTM Model:\n","\n","- **Enhanced Contextual Understanding**: By combining BERT and LSTM, the model captures both deep contextual embeddings and sequence dynamics, offering superior performance over models that might use only one of these methods.\n","- **Flexibility and Adaptability**: The model can be easily adapted to various text classification tasks by fine-tuning on specific datasets, making it highly versatile.\n","- **Robust Performance**: The bidirectional LSTM layer adds an additional layer of context processing, potentially leading to higher accuracy in tasks involving complex linguistic structures.\n","\n","This model represents a robust approach to text classification, harnessing the power of both transformer and recurrent network architectures to deliver high-quality predictions."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T13:23:33.717099Z","iopub.status.busy":"2024-05-11T13:23:33.716756Z","iopub.status.idle":"2024-05-11T13:28:49.868415Z","shell.execute_reply":"2024-05-11T13:28:49.867510Z","shell.execute_reply.started":"2024-05-11T13:23:33.717071Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training Epoch 1: 100%|██████████| 543/543 [01:32<00:00,  5.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.511\n","Training Accuracy: 0.832\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.353\n","Evaluating Accuracy: 0.904\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [01:32<00:00,  5.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.312\n","Training Accuracy: 0.909\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.287\n","Evaluating Accuracy: 0.909\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [01:32<00:00,  5.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.258\n","Training Accuracy: 0.917\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.266\n","Evaluating Accuracy: 0.909\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:06<00:00, 19.26it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.252\n","Final Test Evaluation Accuracy: 0.913\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.52      0.20      0.29       207\n","Offensive Language       0.93      0.96      0.95      2880\n","           Neither       0.87      0.92      0.90       631\n","\n","          accuracy                           0.91      3718\n","         macro avg       0.77      0.70      0.71      3718\n","      weighted avg       0.90      0.91      0.90      3718\n","\n","Test Accuracy: 0.913\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertModel, AdamW\n","\n","# Define the hybrid model\n","class BertLSTMForSequenceClassification(nn.Module):\n","    def __init__(self, bert_model, lstm_hidden_dim, num_labels):\n","        super(BertLSTMForSequenceClassification, self).__init__()\n","        self.bert = bert_model\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","        self.dropout = nn.Dropout(0.1)\n","        self.lstm = nn.LSTM(bidirectional=True, num_layers=1, input_size=bert_model.config.hidden_size,\n","                            hidden_size=lstm_hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(lstm_hidden_dim * 2, num_labels)  # Multiply by 2 for bidirectional LSTM\n","\n","    def forward(self, input_ids, attention_mask):\n","        # BERT encoding\n","        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = bert_outputs.pooler_output\n","        # LSTM processing\n","        lstm_output, _ = self.lstm(pooled_output.unsqueeze(0))\n","        lstm_output = lstm_output.squeeze(0)\n","        # Classification layer\n","        output = self.fc(self.dropout(lstm_output))\n","        return output\n","\n","# Define text cleaning function\n","def clean_text(text):\n","    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load dataset and split\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n","\n","# Tokenization with BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n","\n","# Dataset class\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n","\n","# DataLoader initialization\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Model initialization\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","lstm_hidden_dim = 128\n","num_labels = 3\n","model = BertLSTMForSequenceClassification(bert_model, lstm_hidden_dim, num_labels)\n","optimizer = AdamW(model.parameters(), lr=5e-6)\n","model.to(device)\n","\n","# Training function\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        _, predictions = torch.max(outputs, 1)\n","        total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")\n","\n","# Evaluation function\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            \n","            total_loss += loss.item()\n","            _, predictions = torch.max(outputs, 1)\n","            total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","            \n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions\n","\n","# Main training loop\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
