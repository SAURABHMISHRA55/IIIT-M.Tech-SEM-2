{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Title:** LSTM for Text Classification\n","\n","**Introduction:**\n","This notebook demonstrates the implementation of a Long Short-Term Memory (LSTM) model for text classification using PyTorch. The dataset used is a collection of labeled tweets, and the objective is to classify the tweets into three categories: Hate Speech, Offensive Language, and Neither. The LSTM architecture is employed to capture long-range dependencies in sequential data, followed by classification using a fully connected layer.\n","\n","**Content:**\n","\n","1. **Environment Setup:** The notebook starts by setting up the Python environment and importing necessary libraries.\n","\n","2. **Model Initialization with LSTM:** The LSTM model for text classification is defined in this section. It consists of embedding layers, LSTM layers, and a fully connected layer.\n","\n","3. **Text Cleaning:** A function for cleaning text data is defined to preprocess the tweets, removing URLs, mentions, special characters, and emoticons.\n","\n","4. **Dataset Loading and Splitting:** The labeled tweet dataset is loaded and split into training, validation, and test sets.\n","\n","5. **Tokenization with BERT Tokenizer:** The tweets are tokenized using the BERT tokenizer, which converts text inputs into token IDs.\n","\n","6. **Dataset Class:** A custom dataset class is defined to process the tokenized inputs and corresponding labels.\n","\n","7. **DataLoader Initialization:** DataLoaders are initialized for the training, validation, and test datasets to efficiently load data in batches during model training and evaluation.\n","\n","8. **Model Training:** The training function is defined to train the LSTM model on the training dataset using backpropagation.\n","\n","9. **Evaluation Function:** An evaluation function is defined to assess the model's performance on the validation and test datasets.\n","\n","10. **Main Training Loop:** The main training loop runs for a specified number of epochs, during which the model is trained on the training dataset and evaluated on the validation dataset.\n","\n","11. **Final Evaluation on Test Set:** The trained model's performance is evaluated on the test dataset, and classification metrics such as precision, recall, and F1-score are computed.\n","\n","12. **Conclusion:** The notebook concludes by printing the test accuracy of the LSTM model for text classification.\n","\n","**Conclusion:**\n","This notebook provides a comprehensive implementation of an LSTM for text classification task using PyTorch, showcasing the process from data preprocessing to model evaluation. The model effectively captures long-range dependencies in textual data and achieves competitive performance in classifying tweets into predefined categories. This demonstrates the effectiveness of LSTM networks in handling sequential data for classification tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:04:23.636302Z","iopub.status.busy":"2024-05-09T20:04:23.635326Z","iopub.status.idle":"2024-05-09T20:04:29.581339Z","shell.execute_reply":"2024-05-09T20:04:29.580244Z","shell.execute_reply.started":"2024-05-09T20:04:23.636266Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","\n","# Part 1: Model initialization with LSTM\n","import torch.nn as nn\n","\n","class LSTMForSequenceClassification(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(LSTMForSequenceClassification, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, hidden_dim)\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        output = self.fc(lstm_out[:, -1, :])\n","        return output\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:04:47.486954Z","iopub.status.busy":"2024-05-09T20:04:47.485902Z","iopub.status.idle":"2024-05-09T20:04:47.495005Z","shell.execute_reply":"2024-05-09T20:04:47.493873Z","shell.execute_reply.started":"2024-05-09T20:04:47.486918Z"},"trusted":true},"outputs":[],"source":["# Part 2: Define text cleaning function (remains the same)\n","emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:05:16.616597Z","iopub.status.busy":"2024-05-09T20:05:16.615900Z","iopub.status.idle":"2024-05-09T20:05:17.576542Z","shell.execute_reply":"2024-05-09T20:05:17.575638Z","shell.execute_reply.started":"2024-05-09T20:05:16.616564Z"},"trusted":true},"outputs":[],"source":["# Part 3: Load dataset and split (remains the same)\n","df = pd.read_csv('/kaggle/input/mydata/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:05:28.726514Z","iopub.status.busy":"2024-05-09T20:05:28.726037Z","iopub.status.idle":"2024-05-09T20:05:46.057236Z","shell.execute_reply":"2024-05-09T20:05:46.056335Z","shell.execute_reply.started":"2024-05-09T20:05:28.726474Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38509ba2f40748b6ad2a598a56611d65","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3818c664b94f473eb38a834013659952","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f79b0f3362c468482b701675bcd7e5a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6c1c2a3eba447cbbc58a49c78405369","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Part 4: Tokenization with BERT tokenizer (remains the same)\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:05:57.787443Z","iopub.status.busy":"2024-05-09T20:05:57.786468Z","iopub.status.idle":"2024-05-09T20:05:57.795766Z","shell.execute_reply":"2024-05-09T20:05:57.794778Z","shell.execute_reply.started":"2024-05-09T20:05:57.787380Z"},"trusted":true},"outputs":[],"source":["# Part 5: Dataset class (remains the same)\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:06:09.787320Z","iopub.status.busy":"2024-05-09T20:06:09.786956Z","iopub.status.idle":"2024-05-09T20:06:09.793108Z","shell.execute_reply":"2024-05-09T20:06:09.791962Z","shell.execute_reply.started":"2024-05-09T20:06:09.787293Z"},"trusted":true},"outputs":[],"source":["# Part 6: DataLoader initialization (remains the same)\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:06:20.193302Z","iopub.status.busy":"2024-05-09T20:06:20.192929Z","iopub.status.idle":"2024-05-09T20:06:21.232500Z","shell.execute_reply":"2024-05-09T20:06:21.231212Z","shell.execute_reply.started":"2024-05-09T20:06:20.193271Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LSTMForSequenceClassification(\n","  (embedding): Embedding(30522, 128)\n","  (lstm): LSTM(128, 128, batch_first=True)\n","  (fc): Linear(in_features=128, out_features=3, bias=True)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Part 7: Model initialization with LSTM\n","input_dim = len(tokenizer.get_vocab())\n","hidden_dim = 128\n","output_dim = 3\n","model = LSTMForSequenceClassification(input_dim, hidden_dim, output_dim)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","model.to(device)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:07:51.727669Z","iopub.status.busy":"2024-05-09T20:07:51.726973Z","iopub.status.idle":"2024-05-09T20:07:51.735446Z","shell.execute_reply":"2024-05-09T20:07:51.734465Z","shell.execute_reply.started":"2024-05-09T20:07:51.727634Z"},"trusted":true},"outputs":[],"source":["# Part 8: Training function (remains the same)\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        _, predictions = torch.max(outputs, 1)\n","        total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:08:05.730550Z","iopub.status.busy":"2024-05-09T20:08:05.729642Z","iopub.status.idle":"2024-05-09T20:08:05.742117Z","shell.execute_reply":"2024-05-09T20:08:05.740926Z","shell.execute_reply.started":"2024-05-09T20:08:05.730507Z"},"trusted":true},"outputs":[],"source":["# Part 9: Evaluation function (remains the same)\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            \n","            total_loss += loss.item()\n","            _, predictions = torch.max(outputs, 1)\n","            total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","            \n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T20:08:18.184321Z","iopub.status.busy":"2024-05-09T20:08:18.183557Z","iopub.status.idle":"2024-05-09T20:08:30.948758Z","shell.execute_reply":"2024-05-09T20:08:30.947663Z","shell.execute_reply.started":"2024-05-09T20:08:18.184287Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1: 100%|██████████| 543/543 [00:04<00:00, 128.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.666\n","Training Accuracy: 0.774\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 239.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.666\n","Evaluating Accuracy: 0.774\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [00:03<00:00, 165.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.664\n","Training Accuracy: 0.775\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 251.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.667\n","Evaluating Accuracy: 0.774\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [00:03<00:00, 163.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.664\n","Training Accuracy: 0.775\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 246.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.665\n","Evaluating Accuracy: 0.774\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:00<00:00, 262.26it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.658\n","Final Test Evaluation Accuracy: 0.776\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.00      0.00      0.00       207\n","Offensive Language       0.77      1.00      0.87      2880\n","           Neither       0.00      0.00      0.00       631\n","\n","          accuracy                           0.77      3718\n","         macro avg       0.26      0.33      0.29      3718\n","      weighted avg       0.60      0.77      0.68      3718\n","\n","Test Accuracy: 0.774\n"]},{"name":"stderr","output_type":"stream","text":["\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Part 10: Main training loop (remains the same)\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4975490,"sourceId":8369494,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
