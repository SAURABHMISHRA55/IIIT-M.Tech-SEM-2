{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Title:** BERT for Text Classification\n","\n","**Introduction:**\n","This notebook demonstrates the implementation of a BERT (Bidirectional Encoder Representations from Transformers) model for text classification using PyTorch and the Hugging Face Transformers library. BERT is a powerful pre-trained model that captures bidirectional context in text data and has been widely adopted for various NLP tasks. The dataset used in this notebook consists of labeled tweets, and the objective is to classify the tweets into three categories: Hate Speech, Offensive Language, and Neither.\n","\n","**Content:**\n","\n","1. **Environment Setup:** The notebook starts by setting up the Python environment and importing necessary libraries, including PyTorch, Transformers, and scikit-learn.\n","\n","2. **Text Cleaning:** A function for cleaning text data is defined to preprocess the tweets, removing URLs, mentions, special characters, and emoticons.\n","\n","3. **Dataset Loading and Splitting:** The labeled tweet dataset is loaded and split into training, validation, and test sets.\n","\n","4. **Tokenization with BERT Tokenizer:** The tweets are tokenized using the BERT tokenizer, which converts text inputs into token IDs and attention masks.\n","\n","5. **Dataset Class:** A custom dataset class is defined to process the tokenized inputs and corresponding labels.\n","\n","6. **DataLoader Initialization:** DataLoaders are initialized for the training, validation, and test datasets to efficiently load data in batches during model training and evaluation.\n","\n","7. **Model Initialization:** The BERT model for sequence classification (`BertForSequenceClassification`) is initialized with the pre-trained weights from the `bert-base-uncased` model. The model is then moved to the appropriate device (GPU or CPU).\n","\n","8. **Training Function:** A training function is defined to train the BERT model on the training dataset using backpropagation.\n","\n","9. **Evaluation Function:** An evaluation function is defined to assess the model's performance on the validation and test datasets.\n","\n","10. **Main Training Loop:** The main training loop runs for a specified number of epochs, during which the model is trained on the training dataset and evaluated on the validation dataset.\n","\n","11. **Final Evaluation on Test Set:** The trained model's performance is evaluated on the test dataset, and classification metrics such as precision, recall, and F1-score are computed.\n","\n","12. **Conclusion:** The notebook concludes by printing the test accuracy of the BERT model for text classification.\n","\n","**Conclusion:**\n","This notebook provides a comprehensive implementation of a BERT model for text classification using PyTorch and the Hugging Face Transformers library. BERT effectively captures bidirectional context in textual data and achieves competitive performance in classifying tweets into predefined categories. The notebook demonstrates the simplicity and effectiveness of leveraging pre-trained models like BERT for NLP tasks, making it a valuable tool in natural language processing applications."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-11T09:46:15.656417Z","iopub.status.busy":"2024-05-11T09:46:15.655512Z","iopub.status.idle":"2024-05-11T09:46:15.663294Z","shell.execute_reply":"2024-05-11T09:46:15.662258Z","shell.execute_reply.started":"2024-05-11T09:46:15.656383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:19.479900Z","iopub.status.busy":"2024-05-11T09:46:19.479196Z","iopub.status.idle":"2024-05-11T09:46:19.487264Z","shell.execute_reply":"2024-05-11T09:46:19.486160Z","shell.execute_reply.started":"2024-05-11T09:46:19.479858Z"},"trusted":true},"outputs":[],"source":["emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:23.331727Z","iopub.status.busy":"2024-05-11T09:46:23.331103Z","iopub.status.idle":"2024-05-11T09:46:24.224670Z","shell.execute_reply":"2024-05-11T09:46:24.223813Z","shell.execute_reply.started":"2024-05-11T09:46:23.331697Z"},"trusted":true},"outputs":[],"source":["# Load dataset\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","# Split dataset\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:27.033936Z","iopub.status.busy":"2024-05-11T09:46:27.033566Z","iopub.status.idle":"2024-05-11T09:46:41.347139Z","shell.execute_reply":"2024-05-11T09:46:41.346318Z","shell.execute_reply.started":"2024-05-11T09:46:27.033905Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a6db0fb0b1d4ebb9f4f2340c2f4f775","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0556dcb5a91c452ab4a31d13cf0881ee","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"272f12522d154d40bcfa1f3cbbb353b1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b99e8b7be3d46a8928b7549a51747a0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:44.739692Z","iopub.status.busy":"2024-05-11T09:46:44.739038Z","iopub.status.idle":"2024-05-11T09:46:44.747666Z","shell.execute_reply":"2024-05-11T09:46:44.746100Z","shell.execute_reply.started":"2024-05-11T09:46:44.739660Z"},"trusted":true},"outputs":[],"source":["# Dataset class\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:48.633599Z","iopub.status.busy":"2024-05-11T09:46:48.633242Z","iopub.status.idle":"2024-05-11T09:46:48.638968Z","shell.execute_reply":"2024-05-11T09:46:48.637861Z","shell.execute_reply.started":"2024-05-11T09:46:48.633569Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:51.094292Z","iopub.status.busy":"2024-05-11T09:46:51.093619Z","iopub.status.idle":"2024-05-11T09:46:53.482559Z","shell.execute_reply":"2024-05-11T09:46:53.481771Z","shell.execute_reply.started":"2024-05-11T09:46:51.094260Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"decd022421254646bece36d2ba7f868d","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Model initialization\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n","optimizer = AdamW(model.parameters(), lr=5e-6)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Training function\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        logits = outputs.logits.detach().cpu().numpy()\n","        predictions = np.argmax(logits, axis=-1)\n","        total_accuracy += accuracy_score(labels.cpu().numpy(), predictions)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:46:57.063532Z","iopub.status.busy":"2024-05-11T09:46:57.063197Z","iopub.status.idle":"2024-05-11T09:46:57.072752Z","shell.execute_reply":"2024-05-11T09:46:57.071638Z","shell.execute_reply.started":"2024-05-11T09:46:57.063506Z"},"trusted":true},"outputs":[],"source":["# Evaluation function\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    for batch in tqdm(loader, desc=desc):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        \n","        loss = outputs.loss.item()\n","        total_loss += loss\n","        logits = outputs.logits.detach().cpu().numpy()\n","        predictions = np.argmax(logits, axis=-1)\n","        total_accuracy += accuracy_score(labels.cpu().numpy(), predictions)\n","        \n","        all_predictions.extend(predictions)\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"Validation Loss: {avg_loss:.3f}\")\n","    print(f\"Validation Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T09:47:00.509166Z","iopub.status.busy":"2024-05-11T09:47:00.508442Z","iopub.status.idle":"2024-05-11T09:51:58.414522Z","shell.execute_reply":"2024-05-11T09:51:58.413630Z","shell.execute_reply.started":"2024-05-11T09:47:00.509125Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1: 100%|██████████| 543/543 [01:31<00:00,  5.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.445\n","Training Accuracy: 0.854\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.301\n","Validation Accuracy: 0.902\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [01:30<00:00,  5.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.259\n","Training Accuracy: 0.912\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.252\n","Validation Accuracy: 0.914\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [01:30<00:00,  5.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.224\n","Training Accuracy: 0.922\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.256\n","Validation Accuracy: 0.913\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:06<00:00, 19.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.245\n","Validation Accuracy: 0.911\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.46      0.50      0.48       207\n","Offensive Language       0.95      0.94      0.94      2880\n","           Neither       0.88      0.91      0.90       631\n","\n","          accuracy                           0.91      3718\n","         macro avg       0.77      0.78      0.78      3718\n","      weighted avg       0.91      0.91      0.91      3718\n","\n","Test Accuracy: 0.910\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Main training loop\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
