{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Title:** CNN for Text Classification\n","\n","**Introduction:**\n","This notebook demonstrates the implementation of a Convolutional Neural Network (CNN) for text classification using PyTorch. The dataset used here is labeled tweets, and the task is to classify the tweets into three categories: Hate Speech, Offensive Language, and Neither. The CNN architecture is utilized to learn features from the textual data, followed by classification using a fully connected layer.\n","\n","**Content:**\n","\n","1. **Environment Setup:** The notebook begins with setting up the Python environment and importing necessary libraries.\n","\n","2. **Model Initialization with CNN:** The CNN model for text classification is defined in this section. It consists of embedding layers, convolutional layers, and a fully connected layer.\n","\n","3. **Text Cleaning:** A function for cleaning text data is defined to preprocess the tweets, removing URLs, mentions, special characters, and emoticons.\n","\n","4. **Dataset Loading and Splitting:** The labeled tweet dataset is loaded and split into training, validation, and test sets.\n","\n","5. **Tokenization with BERT Tokenizer:** The tweets are tokenized using the BERT tokenizer, which converts text inputs into token IDs.\n","\n","6. **Dataset Class:** A custom dataset class is defined to process the tokenized inputs and corresponding labels.\n","\n","7. **DataLoader Initialization:** DataLoaders are initialized for the training, validation, and test datasets to efficiently load data in batches during model training and evaluation.\n","\n","8. **Model Training:** The training function is defined to train the CNN model on the training dataset using backpropagation.\n","\n","9. **Evaluation Function:** An evaluation function is defined to assess the model's performance on the validation and test datasets.\n","\n","10. **Main Training Loop:** The main training loop runs for a specified number of epochs, during which the model is trained on the training dataset and evaluated on the validation dataset.\n","\n","11. **Final Evaluation on Test Set:** The trained model's performance is evaluated on the test dataset, and classification metrics such as precision, recall, and F1-score are computed.\n","\n","12. **Conclusion:** The notebook concludes by printing the test accuracy of the CNN model for text classification.\n","\n","**Conclusion:**\n","This notebook provides a comprehensive implementation of a CNN for text classification task using PyTorch, demonstrating the process from data preprocessing to model evaluation. The model achieves competitive performance in classifying tweets into predefined categories, showcasing the effectiveness of CNNs in processing textual data for classification tasks."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-09T21:21:07.693225Z","iopub.status.busy":"2024-05-09T21:21:07.692871Z","iopub.status.idle":"2024-05-09T21:21:08.608107Z","shell.execute_reply":"2024-05-09T21:21:08.606973Z","shell.execute_reply.started":"2024-05-09T21:21:07.693195Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/mydatasetof/labeled_data.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:21:08.610473Z","iopub.status.busy":"2024-05-09T21:21:08.610022Z","iopub.status.idle":"2024-05-09T21:21:15.868819Z","shell.execute_reply":"2024-05-09T21:21:15.867718Z","shell.execute_reply.started":"2024-05-09T21:21:08.610436Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","from transformers import BertTokenizer\n","\n","# Part 1: Model initialization with CNN\n","class CNNForSequenceClassification(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(CNNForSequenceClassification, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, 128)\n","        self.conv1 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3)\n","        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3)\n","        self.fc = nn.Linear(32, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        embedded = embedded.permute(0, 2, 1)  # Change shape for Conv1d\n","        conv_out1 = F.relu(self.conv1(embedded))\n","        conv_out2 = F.relu(self.conv2(conv_out1))\n","        pooled = F.max_pool1d(conv_out2, kernel_size=conv_out2.size(2)).squeeze(2)\n","        output = self.fc(pooled)\n","        return output\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:24:26.669980Z","iopub.status.busy":"2024-05-09T21:24:26.669266Z","iopub.status.idle":"2024-05-09T21:24:26.678322Z","shell.execute_reply":"2024-05-09T21:24:26.677260Z","shell.execute_reply.started":"2024-05-09T21:24:26.669948Z"},"trusted":true},"outputs":[],"source":["# Part 2: Define text cleaning function (remains the same)\n","emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:24:54.689188Z","iopub.status.busy":"2024-05-09T21:24:54.688267Z","iopub.status.idle":"2024-05-09T21:24:55.689868Z","shell.execute_reply":"2024-05-09T21:24:55.688868Z","shell.execute_reply.started":"2024-05-09T21:24:54.689152Z"},"trusted":true},"outputs":[],"source":["# Part 3: Load dataset and split (remains the same)\n","df = pd.read_csv('/kaggle/input/mydatasetof/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:25:07.534888Z","iopub.status.busy":"2024-05-09T21:25:07.533831Z","iopub.status.idle":"2024-05-09T21:25:22.184766Z","shell.execute_reply":"2024-05-09T21:25:22.183478Z","shell.execute_reply.started":"2024-05-09T21:25:07.534839Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d419223e2d846bb9aa6aed8ff3d9506","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26a48f9fb13a4f68bff77aeb927f3532","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb73153da6f144b8ad8170e32feebae5","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b20ca19600142f59d33cf5bdf64a6cc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Part 4: Tokenization with BERT tokenizer (remains the same)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:25:23.840953Z","iopub.status.busy":"2024-05-09T21:25:23.839914Z","iopub.status.idle":"2024-05-09T21:25:23.848914Z","shell.execute_reply":"2024-05-09T21:25:23.847875Z","shell.execute_reply.started":"2024-05-09T21:25:23.840914Z"},"trusted":true},"outputs":[],"source":["# Part 5: Dataset class (remains the same)\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:25:43.466664Z","iopub.status.busy":"2024-05-09T21:25:43.466182Z","iopub.status.idle":"2024-05-09T21:25:43.472526Z","shell.execute_reply":"2024-05-09T21:25:43.471391Z","shell.execute_reply.started":"2024-05-09T21:25:43.466631Z"},"trusted":true},"outputs":[],"source":["# Part 6: DataLoader initialization (remains the same)\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:25:55.723819Z","iopub.status.busy":"2024-05-09T21:25:55.722909Z","iopub.status.idle":"2024-05-09T21:25:56.659607Z","shell.execute_reply":"2024-05-09T21:25:56.657374Z","shell.execute_reply.started":"2024-05-09T21:25:55.723782Z"},"trusted":true},"outputs":[{"data":{"text/plain":["CNNForSequenceClassification(\n","  (embedding): Embedding(30522, 128)\n","  (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n","  (conv2): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n","  (fc): Linear(in_features=32, out_features=3, bias=True)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Part 7: Model initialization with CNN\n","input_dim = len(tokenizer.get_vocab())\n","output_dim = 3\n","model = CNNForSequenceClassification(input_dim, output_dim)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","model.to(device)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:26:07.855417Z","iopub.status.busy":"2024-05-09T21:26:07.854386Z","iopub.status.idle":"2024-05-09T21:26:07.863323Z","shell.execute_reply":"2024-05-09T21:26:07.862287Z","shell.execute_reply.started":"2024-05-09T21:26:07.855386Z"},"trusted":true},"outputs":[],"source":["# Part 8: Training function (remains the same)\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        _, predictions = torch.max(outputs, 1)\n","        total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:26:18.670158Z","iopub.status.busy":"2024-05-09T21:26:18.669537Z","iopub.status.idle":"2024-05-09T21:26:18.679697Z","shell.execute_reply":"2024-05-09T21:26:18.678473Z","shell.execute_reply.started":"2024-05-09T21:26:18.670123Z"},"trusted":true},"outputs":[],"source":["# Part 9: Evaluation function (remains the same)\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            \n","            total_loss += loss.item()\n","            _, predictions = torch.max(outputs, 1)\n","            total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","            \n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T21:26:28.785162Z","iopub.status.busy":"2024-05-09T21:26:28.784256Z","iopub.status.idle":"2024-05-09T21:26:41.609828Z","shell.execute_reply":"2024-05-09T21:26:41.608782Z","shell.execute_reply.started":"2024-05-09T21:26:28.785125Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1: 100%|██████████| 543/543 [00:04<00:00, 120.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.365\n","Training Accuracy: 0.863\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 270.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.295\n","Evaluating Accuracy: 0.897\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [00:03<00:00, 163.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.250\n","Training Accuracy: 0.912\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 270.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.303\n","Evaluating Accuracy: 0.886\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [00:03<00:00, 169.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.201\n","Training Accuracy: 0.928\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 264.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.310\n","Evaluating Accuracy: 0.889\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:00<00:00, 274.56it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.286\n","Final Test Evaluation Accuracy: 0.897\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.49      0.15      0.23       207\n","Offensive Language       0.91      0.97      0.94      2880\n","           Neither       0.87      0.81      0.84       631\n","\n","          accuracy                           0.90      3718\n","         macro avg       0.76      0.64      0.67      3718\n","      weighted avg       0.88      0.90      0.88      3718\n","\n","Test Accuracy: 0.897\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Part 10: Main training loop (remains the same)\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4975736,"sourceId":8369838,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
