{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Overview of the DistilBert + BI-LSTM Classification Model\n","\n","The `DistilBert + BI-LSTM Classification Model` model synergizes the lightweight and efficient DistilBERT with a bidirectional LSTM network to create a powerful text classification system. This model is designed to capture the contextual embeddings provided by DistilBERT and enhance the sequence modeling capabilities with LSTM's to effectively understand and predict sequences over extended contexts. This hybrid model is particularly useful in scenarios where computational efficiency is crucial without significantly sacrificing the depth of contextual understanding required for complex NLP tasks.\n","\n","### Key Components of the Model:\n","\n","1. **DistilBERT Model**:\n","   - Serves as the backbone for generating contextual embeddings from input text. DistilBERT is a smaller, faster version of BERT that retains most of the original model's predictive power, making it suitable for environments with limited computational resources.\n","\n","2. **Bidirectional LSTM**:\n","   - Enhances the sequence processing capabilities by handling information from both past and future contexts simultaneously. This is particularly useful for understanding the context in which words appear within sequences, thereby improving the accuracy of predictions for tasks like sentiment analysis or contextual text classification.\n","\n","3. **Dropout and Linear Layer**:\n","   - Dropout is applied to the output of the LSTM to prevent overfitting, ensuring that the model remains generalizable to new, unseen data. The linear layer then maps the LSTM outputs to the final classification labels.\n","\n","### Implementation Details:\n","\n","1. **Data Preprocessing and Tokenization**:\n","   - Raw text data undergoes a cleaning process to standardize it by removing URLs, user mentions, and special characters, ensuring that the DistilBERT tokenizer processes only relevant textual content.\n","   - The DistilBertTokenizer is used to tokenize texts, which are then padded to a uniform length to ensure consistent input size.\n","\n","2. **Dataset and DataLoader**:\n","   - A custom `TweetDataset` class is utilized to manage the tokenized data and corresponding labels. This setup facilitates efficient data loading during training and evaluation through the PyTorch `DataLoader`.\n","\n","3. **Model Configuration**:\n","   - The model integrates DistilBERT with an LSTM layer that is specifically configured to process the output embeddings from DistilBERT. The model operates in a bidirectional manner to capture dependencies and context from both directions of a sequence.\n","\n","4. **Training and Evaluation**:\n","   - The model is trained over multiple epochs where it learns by minimizing the cross-entropy loss between predicted and actual labels, using the AdamW optimizer for effective backpropagation and weight adjustment.\n","   - During evaluation, the model's performance is assessed on validation and test sets to ensure its effectiveness and ability to generalize across different datasets.\n","\n","5. **GPU Utilization for Enhanced Performance**:\n","   - The model checks for GPU availability and utilizes it if available, significantly enhancing computational speed and efficiency necessary for training and inference phases.\n","\n","### Benefits of Using DistilBertLSTM:\n","\n","- **Efficient Computation**: By using DistilBERT, the model achieves faster computation times and requires less memory, making it feasible for deployment in resource-constrained environments.\n","- **Enhanced Sequence Modeling**: The addition of a bidirectional LSTM allows the model to effectively understand and utilize the context around each word in a sentence, leading to more accurate predictions, especially in tasks requiring a deep understanding of textual context.\n","- **Adaptability**: This model can be easily adapted and fine-tuned for various NLP tasks, including but not limited to sentiment analysis, hate speech detection, and other forms of text classification.\n","\n","This hybrid model represents an optimal approach for applications needing a balance between performance and computational efficiency, offering robust text classification capabilities enhanced by deep contextual embeddings and sophisticated sequence modeling."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T13:31:38.699187Z","iopub.status.busy":"2024-05-11T13:31:38.698805Z","iopub.status.idle":"2024-05-11T13:34:29.561671Z","shell.execute_reply":"2024-05-11T13:34:29.560691Z","shell.execute_reply.started":"2024-05-11T13:31:38.699154Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training Epoch 1: 100%|██████████| 543/543 [00:47<00:00, 11.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.461\n","Training Accuracy: 0.839\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:03<00:00, 36.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.308\n","Evaluating Accuracy: 0.902\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [00:47<00:00, 11.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.280\n","Training Accuracy: 0.909\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:03<00:00, 35.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.262\n","Evaluating Accuracy: 0.911\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [00:47<00:00, 11.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.241\n","Training Accuracy: 0.918\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:03<00:00, 35.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.253\n","Evaluating Accuracy: 0.907\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:03<00:00, 36.91it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.246\n","Final Test Evaluation Accuracy: 0.915\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.51      0.36      0.42       207\n","Offensive Language       0.94      0.96      0.95      2880\n","           Neither       0.89      0.89      0.89       631\n","\n","          accuracy                           0.91      3718\n","         macro avg       0.78      0.73      0.75      3718\n","      weighted avg       0.91      0.91      0.91      3718\n","\n","Test Accuracy: 0.914\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n","\n","# Define the hybrid model\n","class DistilBertLSTMForSequenceClassification(nn.Module):\n","    def __init__(self, distilbert_model, lstm_hidden_dim, num_labels):\n","        super(DistilBertLSTMForSequenceClassification, self).__init__()\n","        self.distilbert = distilbert_model\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","        self.dropout = nn.Dropout(0.1)\n","        self.lstm = nn.LSTM(bidirectional=True, num_layers=1, input_size=distilbert_model.config.dim,\n","                            hidden_size=lstm_hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(lstm_hidden_dim * 2, num_labels)  # Multiply by 2 for bidirectional LSTM\n","\n","    def forward(self, input_ids, attention_mask):\n","        # DistilBERT encoding\n","        distilbert_outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = distilbert_outputs.last_hidden_state[:, 0, :]  # Use CLS token for classification\n","        # LSTM processing\n","        lstm_output, _ = self.lstm(pooled_output.unsqueeze(0))\n","        lstm_output = lstm_output.squeeze(0)\n","        # Classification layer\n","        output = self.fc(self.dropout(lstm_output))\n","        return output\n","\n","# Define text cleaning function\n","def clean_text(text):\n","    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n","\n","# Load dataset and split\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n","\n","# Tokenization with DistilBERT tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n","\n","# Dataset class\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n","\n","# DataLoader initialization\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Model initialization\n","distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","lstm_hidden_dim = 128\n","num_labels = 3\n","model = DistilBertLSTMForSequenceClassification(distilbert_model, lstm_hidden_dim, num_labels)\n","optimizer = AdamW(model.parameters(), lr=5e-6)\n","model.to(device)\n","\n","# Training function\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        _, predictions = torch.max(outputs, 1)\n","        total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")\n","\n","# Evaluation function\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            \n","            total_loss += loss.item()\n","            _, predictions = torch.max(outputs, 1)\n","            total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","            \n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions\n","\n","# Main training loop\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
