{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Overview of Merging BERT and CNN Models\n","\n","The merging of BERT and CNN models into a cohesive architecture aims to harness the deep contextual understanding of BERT with the spatial feature extraction capabilities of CNNs for text classification. This integration can potentially enhance performance on tasks like sentiment analysis or hate speech detection, where both context and local textual features are crucial.\n","\n","### Steps in Merging BERT and CNN:\n","\n","1. **Data Preprocessing and Tokenization**:\n","   - **Text Cleaning**: Textual data is cleaned to remove URLs, user mentions, emoticons, and numeric characters, which are irrelevant for understanding the semantic meaning of texts.\n","   - **Tokenization**: Using BERT's tokenizer, the cleaned text is converted into token IDs, which are numerical representations understandable by the BERT model. These tokens are padded to a fixed length to maintain uniformity across inputs.\n","\n","2. **Dataset Preparation**:\n","   - A custom `Dataset` class handles the storage of tokenized text and labels, facilitating easy batch loading during training and evaluation through PyTorch’s `DataLoader`.\n","\n","3. **Model Architecture (BertCNN)**:\n","   - **BERT Layer**: The BERT model serves as the feature extractor, where the embeddings for each token are generated. These embeddings are rich in contextual information, capturing both the meaning of each word in the context of the surrounding words.\n","   - **CNN Layers**: Following BERT, a series of convolutional layers are applied. These layers are designed to extract higher-level features from the BERT embeddings. The convolutional operations can capture patterns across different parts of the sentence, which are essential for understanding complex linguistic constructs like negations or conditionals.\n","   - **Pooling and Classification**: After convolution, a max pooling layer reduces the dimensionality of the feature maps, focusing only on the most relevant features. The pooled output is then passed to a fully connected layer that maps these features to the target classes.\n","\n","4. **Training and Evaluation**:\n","   - **Training Loop**: In each epoch, the model undergoes training where it learns by adjusting weights to minimize the loss between the predicted and actual class labels. Gradients are computed for each batch, and weights are updated using the AdamW optimizer.\n","   - **Evaluation Loop**: Post training, the model is evaluated on the validation and test sets to monitor performance and avoid overfitting. Performance metrics such as loss and accuracy provide insights into model effectiveness.\n","\n","5. **Model Deployment**:\n","   - The model, once trained and validated, can be deployed for inferencing, where it can classify new, unseen text data.\n","\n","### Explanation of the Merged Code:\n","\n","The merged code defines an end-to-end workflow from data preprocessing, model definition, training, and evaluation:\n","\n","- **Data Loading and Cleaning**: Data is loaded from a CSV file and cleaned using a predefined function that strips unnecessary characters and normalizes the text.\n","- **Tokenization**: Texts are converted into tokenized formats suitable for BERT.\n","- **Dataset and DataLoader Setup**: Tokenized texts are encapsulated in a custom dataset class, which is then used with DataLoader for efficient batch processing during model training.\n","- **Model Definition (BertCNN)**:\n","  - The model integrates BERT for obtaining deep contextual embeddings of the text.\n","  - Convolutional layers process these embeddings to capture spatial dependencies.\n","  - The output through the convolutional layers undergoes pooling and is finally fed into a dense layer for classification.\n","- **Training and Evaluation**:\n","  - Detailed training and evaluation functions are defined, incorporating loss computation, backpropagation, and accuracy calculation.\n","  - The training function includes gradient zeroing and optimizer steps, essential for correct weight updates.\n","  - Evaluation assesses the model's performance on unseen data, crucial for testing the model's generalizability.\n","\n","This integration effectively combines the nuanced understanding of text provided by BERT with the robust feature extraction capabilities of CNNs, aiming to create a powerful tool for text classification tasks."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:40:08.518603Z","iopub.status.busy":"2024-05-11T12:40:08.518251Z","iopub.status.idle":"2024-05-11T12:42:21.380647Z","shell.execute_reply":"2024-05-11T12:42:21.379477Z","shell.execute_reply.started":"2024-05-11T12:40:08.518572Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"881c2353b3114e7489f1fb2143a1cf66","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e3bcf8d8e33463a869386e8e66de442","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acbbf77a45824044aee31b0df5488985","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b18a176e6a044049b9979c92905893b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1f84bce54e94b0994bb9b8979319a0a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training Epoch 1: 100%|██████████| 543/543 [00:31<00:00, 17.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.460\n","Training Accuracy: 0.837\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.332\n","Evaluating Accuracy: 0.879\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [00:29<00:00, 18.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.320\n","Training Accuracy: 0.881\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 19.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.301\n","Evaluating Accuracy: 0.890\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [00:29<00:00, 18.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.291\n","Training Accuracy: 0.891\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.284\n","Evaluating Accuracy: 0.898\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:06<00:00, 19.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.271\n","Final Test Evaluation Accuracy: 0.898\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.54      0.33      0.41       207\n","Offensive Language       0.93      0.95      0.94      2880\n","           Neither       0.82      0.86      0.84       631\n","\n","          accuracy                           0.90      3718\n","         macro avg       0.76      0.71      0.73      3718\n","      weighted avg       0.89      0.90      0.89      3718\n","\n","Test Accuracy: 0.899\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import re\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","\n","# Data Preprocessing and Tokenization\n","def clean_text(text):\n","    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n","\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n","\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# BERT + CNN Integrated Model\n","class BertCNN(nn.Module):\n","    def __init__(self, bert_model, num_classes):\n","        super(BertCNN, self).__init__()\n","        self.bert = bert_model\n","        self.conv1 = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)  # Adjust parameters as needed\n","        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n","        self.fc = nn.Linear(128, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        with torch.no_grad():\n","            outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        # outputs[0] = [batch_size, seq_length, hidden_size]\n","        x = outputs[0].permute(0, 2, 1)  # Change to [batch_size, hidden_size, seq_length]\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool1d(x, kernel_size=x.size(2)).squeeze(2)\n","        x = self.fc(x)\n","        return x\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","model = BertCNN(bert_model, num_classes=3)\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Training Function\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        predictions = torch.argmax(outputs, dim=1)\n","        total_accuracy += (predictions == labels).sum().item() / labels.size(0)\n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")\n","\n","# Evaluation Function\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            total_loss += loss.item()\n","            predictions = torch.argmax(outputs, dim=1)\n","            total_accuracy += (predictions == labels).sum().item() / labels.size(0)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    return all_labels, all_predictions\n","\n","# Main Training Loop\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final Evaluation on Test Set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
