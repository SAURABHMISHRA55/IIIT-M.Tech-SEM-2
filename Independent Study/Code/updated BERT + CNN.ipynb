{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Overview of the Optimized BERT and CNN Integration\n","\n","The integration of BERT and CNN models was refined to optimize performance, simplify architecture, enhance memory management, and improve training dynamics. The changes are designed to streamline operations, reduce computational overhead, and better harness the strengths of both BERT and CNN architectures for text classification tasks.\n","\n","### Detailed Changes Made:\n","\n","1. **Simplified Model Architecture**:\n","    - **Last Hidden Layer Output**: The model now utilizes only the last hidden layer output of BERT instead of combining outputs from multiple layers. This change reduces the complexity of the input to the CNN, potentially reducing overfitting and computational demands.\n","    - **Simplified CNN Structure**: The CNN architecture was simplified by reducing the number of convolutional layers and using an adaptive max pooling layer. This focuses the model on extracting the most salient features from the BERT output, enhancing processing efficiency.\n","\n","2. **Optimized Memory Management**:\n","    - **Garbage Collection and CUDA Cache Clearing**: `gc.collect()` and `torch.cuda.empty_cache()` are used within the training and evaluation loops. These commands help in managing GPU memory more efficiently, preventing out-of-memory errors which are common in large model training scenarios.\n","\n","3. **Streamlined Data Preprocessing**:\n","    - **Standardized Text Cleaning**: The text cleaning function was standardized to remove URLs, user mentions, and emoticons consistently across the dataset. This standardization helps in reducing noise in the input data, focusing the model training on relevant textual content.\n","\n","4. **Updated Training and Evaluation Loops**:\n","    - **Dynamic Progress Updates**: Progress updates during training and evaluation now include real-time feedback on loss and accuracy for each batch, enhancing the visibility of the model’s performance during training.\n","    - **Gradient Clipping**: Implemented to prevent exploding gradients, which is crucial for maintaining stable training dynamics especially when working with deep neural networks.\n","\n","5. **Detailed Batch and DataLoader Handling**:\n","    - **Batch Data Handling in GPU Memory**: Explicit handling of batch data to optimize performance and ensure efficient data processing. This includes systematically moving batch data to the GPU and clearing it post-usage.\n","    - **RandomSampler and SequentialSampler**: Used for the training and validation datasets, respectively. This ensures that the model sees randomized data during training and sequential data during validation, aiding in robust learning and consistent validation.\n","\n","6. **Model Saving and Loading**:\n","    - **Conditional Model Saving**: The model's weights are saved only when there is an improvement in validation loss. This not only saves storage space but also ensures that the training can continue from the best state if needed.\n","    - **Loading Pre-trained Weights**: Functionality to load pre-trained weights if available, which can accelerate convergence and improve model robustness by leveraging previously learned weights.\n","\n","7. **Class Weight Handling**:\n","    - **Imbalanced Data Handling**: If the dataset is imbalanced, class weights are calculated and applied in the loss function to prioritize learning from underrepresented classes, aiming to improve model fairness and accuracy.\n","\n","### Impact on Accuracy:\n","\n","These changes collectively contribute to improving the model’s accuracy in several ways:\n","\n","- **Reduced Overfitting**: By simplifying the input from BERT to CNN and streamlining the CNN architecture, the model is less likely to overfit to noise and more likely to generalize better to unseen data.\n","- **Efficient Memory Management**: Prevents interruptions in training due to memory issues, allowing the model to train more consistently and effectively.\n","- **Focused Feature Learning**: With cleaner data and strategic feature extraction via CNN, the model focuses on the most informative aspects of the data, improving its ability to make accurate classifications.\n","- **Stable Training Dynamics**: Gradient clipping and efficient batch handling promote more stable updates during training, which is crucial for achieving high performance in deep learning models.\n","\n","These optimizations ensure that the model not only trains more efficiently but also achieves higher accuracy by focusing on meaningful features and maintaining stability throughout the training process."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:50:37.309938Z","iopub.status.busy":"2024-05-11T12:50:37.309415Z","iopub.status.idle":"2024-05-11T13:02:17.568964Z","shell.execute_reply":"2024-05-11T13:02:17.567999Z","shell.execute_reply.started":"2024-05-11T12:50:37.309906Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training Epoch 1: 100%|██████████| 543/543 [03:41<00:00,  2.45it/s]\n","Evaluating: 100%|██████████| 117/117 [00:06<00:00, 19.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.280\n","Evaluating Accuracy: 0.903\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [03:37<00:00,  2.50it/s]\n","Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.263\n","Evaluating Accuracy: 0.911\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [03:41<00:00,  2.45it/s]\n","Evaluating: 100%|██████████| 117/117 [00:06<00:00, 18.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.317\n","Evaluating Accuracy: 0.909\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:06<00:00, 19.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.295\n","Final Test Evaluation Accuracy: 0.910\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.48      0.39      0.43       207\n","Offensive Language       0.95      0.95      0.95      2880\n","           Neither       0.86      0.92      0.89       631\n","\n","          accuracy                           0.91      3718\n","         macro avg       0.76      0.75      0.75      3718\n","      weighted avg       0.91      0.91      0.91      3718\n","\n","Test Accuracy: 0.911\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","import gc\n","\n","# Data Preprocessing and Tokenization\n","def clean_text(text):\n","    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n","\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=SequentialSampler(val_dataset))\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=SequentialSampler(test_dataset))\n","\n","# BERT + CNN Integrated Model\n","class BertCNN(nn.Module):\n","    def __init__(self, bert_model, num_classes):\n","        super(BertCNN, self).__init__()\n","        self.bert = bert_model\n","        self.conv1 = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n","        self.pool = nn.AdaptiveMaxPool1d(1)\n","        self.fc = nn.Linear(256, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        x = outputs.last_hidden_state.permute(0, 2, 1)\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x).squeeze(2)\n","        x = self.fc(x)\n","        return x\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","model = BertCNN(bert_model, num_classes=3)\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Gradient clipping function\n","def clip_gradients(model, max_norm=1.0):\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","\n","# Training Function\n","def train(epoch):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        clip_gradients(model)\n","        optimizer.step()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","# Evaluation Function\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            total_loss += loss.item()\n","            predictions = torch.argmax(outputs, dim=1)\n","            total_accuracy += (predictions == labels).sum().item() / labels.size(0)\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    return all_labels, all_predictions\n","\n","# Main Training Loop\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final Evaluation on Test Set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
