{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Title:** Bidirectional LSTM for Text Classification\n","\n","**Introduction:**\n","This notebook demonstrates the implementation of a Bidirectional Long Short-Term Memory (LSTM) model for text classification using PyTorch. The dataset used comprises labeled tweets, and the objective is to classify the tweets into three categories: Hate Speech, Offensive Language, and Neither. The Bidirectional LSTM architecture is employed to capture both past and future context in sequential data, followed by classification using a fully connected layer.\n","\n","**Content:**\n","\n","1. **Environment Setup:** The notebook begins with setting up the Python environment and importing necessary libraries.\n","\n","2. **Model Initialization with Bidirectional LSTM:** The Bidirectional LSTM model for text classification is defined in this section. It consists of embedding layers, Bidirectional LSTM layers, and a fully connected layer.\n","\n","3. **Text Cleaning:** A function for cleaning text data is defined to preprocess the tweets, removing URLs, mentions, special characters, and emoticons.\n","\n","4. **Dataset Loading and Splitting:** The labeled tweet dataset is loaded and split into training, validation, and test sets.\n","\n","5. **Tokenization with BERT Tokenizer:** The tweets are tokenized using the BERT tokenizer, which converts text inputs into token IDs.\n","\n","6. **Dataset Class:** A custom dataset class is defined to process the tokenized inputs and corresponding labels.\n","\n","7. **DataLoader Initialization:** DataLoaders are initialized for the training, validation, and test datasets to efficiently load data in batches during model training and evaluation.\n","\n","8. **Model Training:** The training function is defined to train the Bidirectional LSTM model on the training dataset using backpropagation.\n","\n","9. **Evaluation Function:** An evaluation function is defined to assess the model's performance on the validation and test datasets.\n","\n","10. **Main Training Loop:** The main training loop runs for a specified number of epochs, during which the model is trained on the training dataset and evaluated on the validation dataset.\n","\n","11. **Final Evaluation on Test Set:** The trained model's performance is evaluated on the test dataset, and classification metrics such as precision, recall, and F1-score are computed.\n","\n","12. **Conclusion:** The notebook concludes by printing the test accuracy of the Bidirectional LSTM model for text classification.\n","\n","**Conclusion:**\n","This notebook provides a comprehensive implementation of a Bidirectional LSTM for text classification task using PyTorch, showcasing the process from data preprocessing to model evaluation. The model effectively captures both past and future context in textual data and achieves competitive performance in classifying tweets into predefined categories. This demonstrates the effectiveness of Bidirectional LSTMs in handling sequential data for classification tasks."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-11T12:02:15.557956Z","iopub.status.busy":"2024-05-11T12:02:15.557247Z","iopub.status.idle":"2024-05-11T12:02:15.907021Z","shell.execute_reply":"2024-05-11T12:02:15.906129Z","shell.execute_reply.started":"2024-05-11T12:02:15.557923Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/dataset/labeled_data.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:17.872485Z","iopub.status.busy":"2024-05-11T12:02:17.871536Z","iopub.status.idle":"2024-05-11T12:02:24.075470Z","shell.execute_reply":"2024-05-11T12:02:24.074489Z","shell.execute_reply.started":"2024-05-11T12:02:17.872451Z"},"trusted":true},"outputs":[],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","from transformers import BertTokenizer\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:27.123051Z","iopub.status.busy":"2024-05-11T12:02:27.122047Z","iopub.status.idle":"2024-05-11T12:02:27.130442Z","shell.execute_reply":"2024-05-11T12:02:27.129369Z","shell.execute_reply.started":"2024-05-11T12:02:27.123014Z"},"trusted":true},"outputs":[],"source":["# Part 1: Model initialization with Bidirectional LSTM\n","class BiLSTMForSequenceClassification(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(BiLSTMForSequenceClassification, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, hidden_dim)\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply hidden_dim by 2 for bidirectional LSTM\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        # Concatenate the hidden states of the forward and backward LSTM\n","        combined_out = torch.cat((lstm_out[:, -1, :hidden_dim], lstm_out[:, 0, hidden_dim:]), dim=1)\n","        output = self.fc(combined_out)\n","        return output"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:29.460713Z","iopub.status.busy":"2024-05-11T12:02:29.459802Z","iopub.status.idle":"2024-05-11T12:02:29.496739Z","shell.execute_reply":"2024-05-11T12:02:29.495690Z","shell.execute_reply.started":"2024-05-11T12:02:29.460663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["# Part 2: Define text cleaning function (remains the same)\n","emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:31.935216Z","iopub.status.busy":"2024-05-11T12:02:31.934405Z","iopub.status.idle":"2024-05-11T12:02:32.813731Z","shell.execute_reply":"2024-05-11T12:02:32.812724Z","shell.execute_reply.started":"2024-05-11T12:02:31.935180Z"},"trusted":true},"outputs":[],"source":["# Part 3: Load dataset and split (remains the same)\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:34.774631Z","iopub.status.busy":"2024-05-11T12:02:34.773723Z","iopub.status.idle":"2024-05-11T12:02:48.595711Z","shell.execute_reply":"2024-05-11T12:02:48.594724Z","shell.execute_reply.started":"2024-05-11T12:02:34.774593Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d031813e1c014b90abde0135bd5cbaa3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9623284a4f4b4e17905ee9aa14cc40a7","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c13daa1ec454be9bf30670ae330928d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a16f53682c54795bea67b639d6385ed","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Part 4: Tokenization with BERT tokenizer (remains the same)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:51.172067Z","iopub.status.busy":"2024-05-11T12:02:51.171455Z","iopub.status.idle":"2024-05-11T12:02:51.179520Z","shell.execute_reply":"2024-05-11T12:02:51.178558Z","shell.execute_reply.started":"2024-05-11T12:02:51.172034Z"},"trusted":true},"outputs":[],"source":["# Part 5: Dataset class (remains the same)\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:54.210737Z","iopub.status.busy":"2024-05-11T12:02:54.210381Z","iopub.status.idle":"2024-05-11T12:02:54.216185Z","shell.execute_reply":"2024-05-11T12:02:54.215219Z","shell.execute_reply.started":"2024-05-11T12:02:54.210708Z"},"trusted":true},"outputs":[],"source":["# Part 6: DataLoader initialization (remains the same)\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:02:56.786074Z","iopub.status.busy":"2024-05-11T12:02:56.785188Z","iopub.status.idle":"2024-05-11T12:02:57.775471Z","shell.execute_reply":"2024-05-11T12:02:57.774471Z","shell.execute_reply.started":"2024-05-11T12:02:56.786039Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BiLSTMForSequenceClassification(\n","  (embedding): Embedding(30522, 128)\n","  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=256, out_features=3, bias=True)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Part 7: Model initialization with Bidirectional LSTM\n","input_dim = len(tokenizer.get_vocab())\n","hidden_dim = 128\n","output_dim = 3\n","model = BiLSTMForSequenceClassification(input_dim, hidden_dim, output_dim)  # Changed model class to BiLSTMForSequenceClassification\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","model.to(device)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:03:21.672903Z","iopub.status.busy":"2024-05-11T12:03:21.672160Z","iopub.status.idle":"2024-05-11T12:03:21.680718Z","shell.execute_reply":"2024-05-11T12:03:21.679417Z","shell.execute_reply.started":"2024-05-11T12:03:21.672869Z"},"trusted":true},"outputs":[],"source":["# Part 8: Training function (remains the same)\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids)\n","        loss = nn.CrossEntropyLoss()(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        _, predictions = torch.max(outputs, 1)\n","        total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:03:23.712194Z","iopub.status.busy":"2024-05-11T12:03:23.711834Z","iopub.status.idle":"2024-05-11T12:03:23.722237Z","shell.execute_reply":"2024-05-11T12:03:23.720384Z","shell.execute_reply.started":"2024-05-11T12:03:23.712164Z"},"trusted":true},"outputs":[],"source":["# Part 9: Evaluation function (remains the same)\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=desc):\n","            input_ids = batch['input_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids)\n","            loss = nn.CrossEntropyLoss()(outputs, labels)\n","            \n","            total_loss += loss.item()\n","            _, predictions = torch.max(outputs, 1)\n","            total_accuracy += torch.sum(predictions == labels).item() / len(labels)\n","            \n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"{desc} Loss: {avg_loss:.3f}\")\n","    print(f\"{desc} Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T12:03:43.011743Z","iopub.status.busy":"2024-05-11T12:03:43.011369Z","iopub.status.idle":"2024-05-11T12:03:55.607970Z","shell.execute_reply":"2024-05-11T12:03:55.606968Z","shell.execute_reply.started":"2024-05-11T12:03:43.011715Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1: 100%|██████████| 543/543 [00:03<00:00, 147.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.430\n","Training Accuracy: 0.845\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 247.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.337\n","Evaluating Accuracy: 0.884\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [00:03<00:00, 157.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.304\n","Training Accuracy: 0.893\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 246.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.302\n","Evaluating Accuracy: 0.896\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [00:03<00:00, 156.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.254\n","Training Accuracy: 0.912\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:00<00:00, 228.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating Loss: 0.299\n","Evaluating Accuracy: 0.898\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:00<00:00, 250.22it/s]"]},{"name":"stdout","output_type":"stream","text":["Final Test Evaluation Loss: 0.285\n","Final Test Evaluation Accuracy: 0.900\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.48      0.21      0.29       207\n","Offensive Language       0.93      0.94      0.94      2880\n","           Neither       0.81      0.92      0.86       631\n","\n","          accuracy                           0.90      3718\n","         macro avg       0.74      0.69      0.70      3718\n","      weighted avg       0.89      0.90      0.89      3718\n","\n","Test Accuracy: 0.899\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Part 10: Main training loop (remains the same)\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
