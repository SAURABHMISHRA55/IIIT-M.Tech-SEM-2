{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Title:** DistilBERT for Text Classification\n","\n","**Introduction:**\n","This notebook demonstrates the implementation of a DistilBERT model for text classification using PyTorch and the Hugging Face Transformers library. DistilBERT is a smaller, faster, and lighter version of the BERT model while retaining most of its performance. The dataset used in this notebook consists of labeled tweets, and the objective is to classify the tweets into three categories: Hate Speech, Offensive Language, and Neither.\n","\n","**Content:**\n","\n","1. **Environment Setup:** The notebook begins by setting up the Python environment and importing necessary libraries, including PyTorch, Transformers, and scikit-learn.\n","\n","2. **Text Cleaning:** A function for cleaning text data is defined to preprocess the tweets, removing URLs, mentions, special characters, and emoticons.\n","\n","3. **Dataset Loading and Splitting:** The labeled tweet dataset is loaded and split into training, validation, and test sets.\n","\n","4. **Tokenization with DistilBERT Tokenizer:** The tweets are tokenized using the DistilBERT tokenizer, which converts text inputs into token IDs and attention masks.\n","\n","5. **Dataset Class:** A custom dataset class is defined to process the tokenized inputs and corresponding labels.\n","\n","6. **DataLoader Initialization:** DataLoaders are initialized for the training, validation, and test datasets to efficiently load data in batches during model training and evaluation.\n","\n","7. **Model Initialization:** The DistilBERT model for sequence classification (`DistilBertForSequenceClassification`) is initialized with the pre-trained weights from the `distilbert-base-uncased` model. The model is then moved to the appropriate device (GPU or CPU).\n","\n","8. **Training Function:** A training function is defined to train the DistilBERT model on the training dataset using backpropagation.\n","\n","9. **Evaluation Function:** An evaluation function is defined to assess the model's performance on the validation and test datasets.\n","\n","10. **Main Training Loop:** The main training loop runs for a specified number of epochs, during which the model is trained on the training dataset and evaluated on the validation dataset.\n","\n","11. **Final Evaluation on Test Set:** The trained model's performance is evaluated on the test dataset, and classification metrics such as precision, recall, and F1-score are computed.\n","\n","12. **Conclusion:** The notebook concludes by printing the test accuracy of the DistilBERT model for text classification.\n","\n","**Conclusion:**\n","This notebook provides a complete implementation of a DistilBERT model for text classification using PyTorch and the Hugging Face Transformers library. DistilBERT offers a more computationally efficient alternative to BERT while maintaining competitive performance. The notebook demonstrates the effectiveness of leveraging pre-trained models like DistilBERT for NLP tasks, making it a valuable tool in natural language processing applications."]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-11T10:55:37.315057Z","iopub.status.busy":"2024-05-11T10:55:37.314370Z","iopub.status.idle":"2024-05-11T10:55:37.321526Z","shell.execute_reply":"2024-05-11T10:55:37.320624Z","shell.execute_reply.started":"2024-05-11T10:55:37.315023Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:55:43.619872Z","iopub.status.busy":"2024-05-11T10:55:43.619494Z","iopub.status.idle":"2024-05-11T10:55:43.628210Z","shell.execute_reply":"2024-05-11T10:55:43.627170Z","shell.execute_reply.started":"2024-05-11T10:55:43.619841Z"},"trusted":true},"outputs":[],"source":["emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '3', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp', ':-|', ':->', ':-<', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'https?://[^\\s]+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'\\d+', '', text)\n","    for emoticon in emoticons:\n","        text = text.replace(emoticon, '')\n","    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n","    text = re.sub(r\"([?.!,¿])\", r\" \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text.strip()\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:55:46.694628Z","iopub.status.busy":"2024-05-11T10:55:46.693946Z","iopub.status.idle":"2024-05-11T10:55:47.523995Z","shell.execute_reply":"2024-05-11T10:55:47.523208Z","shell.execute_reply.started":"2024-05-11T10:55:46.694590Z"},"trusted":true},"outputs":[],"source":["# Load dataset\n","df = pd.read_csv('/kaggle/input/dataset/labeled_data.csv')\n","df['tweet'] = df['tweet'].apply(clean_text)\n","\n","# Split dataset\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], test_size=0.3, random_state=42)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:55:49.895791Z","iopub.status.busy":"2024-05-11T10:55:49.895037Z","iopub.status.idle":"2024-05-11T10:56:02.927255Z","shell.execute_reply":"2024-05-11T10:56:02.926223Z","shell.execute_reply.started":"2024-05-11T10:55:49.895758Z"},"trusted":true},"outputs":[],"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:56:05.582070Z","iopub.status.busy":"2024-05-11T10:56:05.581714Z","iopub.status.idle":"2024-05-11T10:56:05.589755Z","shell.execute_reply":"2024-05-11T10:56:05.588828Z","shell.execute_reply.started":"2024-05-11T10:56:05.582042Z"},"trusted":true},"outputs":[],"source":["# Dataset class\n","class TweetDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TweetDataset(train_encodings, train_labels.tolist())\n","test_dataset = TweetDataset(test_encodings, test_labels.tolist())\n","val_dataset = TweetDataset(val_encodings, val_labels.tolist())"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:56:08.719234Z","iopub.status.busy":"2024-05-11T10:56:08.718886Z","iopub.status.idle":"2024-05-11T10:56:08.739601Z","shell.execute_reply":"2024-05-11T10:56:08.738317Z","shell.execute_reply.started":"2024-05-11T10:56:08.719208Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:56:11.034320Z","iopub.status.busy":"2024-05-11T10:56:11.033348Z","iopub.status.idle":"2024-05-11T10:56:11.503727Z","shell.execute_reply":"2024-05-11T10:56:11.502572Z","shell.execute_reply.started":"2024-05-11T10:56:11.034278Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Model initialization\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)  # Adjust num_labels according to your classification problem\n","optimizer = AdamW(model.parameters(), lr=5e-6)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Training function\n","def train(epoch):\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        logits = outputs.logits.detach().cpu().numpy()\n","        predictions = np.argmax(logits, axis=-1)\n","        total_accuracy += accuracy_score(labels.cpu().numpy(), predictions)\n","    \n","    avg_loss = total_loss / len(train_loader)\n","    avg_accuracy = total_accuracy / len(train_loader)\n","    print(f\"Training Loss: {avg_loss:.3f}\")\n","    print(f\"Training Accuracy: {avg_accuracy:.3f}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:56:18.706902Z","iopub.status.busy":"2024-05-11T10:56:18.706539Z","iopub.status.idle":"2024-05-11T10:56:18.718786Z","shell.execute_reply":"2024-05-11T10:56:18.717673Z","shell.execute_reply.started":"2024-05-11T10:56:18.706873Z"},"trusted":true},"outputs":[],"source":["# Evaluation function\n","def evaluate(loader, desc=\"Evaluating\"):\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    all_predictions, all_labels = [], []\n","    \n","    for batch in tqdm(loader, desc=desc):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        \n","        loss = outputs.loss.item()\n","        total_loss += loss\n","        logits = outputs.logits.detach().cpu().numpy()\n","        predictions = np.argmax(logits, axis=-1)\n","        total_accuracy += accuracy_score(labels.cpu().numpy(), predictions)\n","        \n","        all_predictions.extend(predictions)\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_accuracy = total_accuracy / len(loader)\n","    print(f\"Validation Loss: {avg_loss:.3f}\")\n","    print(f\"Validation Accuracy: {avg_accuracy:.3f}\")\n","    \n","    return all_labels, all_predictions"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T10:56:21.346855Z","iopub.status.busy":"2024-05-11T10:56:21.346512Z","iopub.status.idle":"2024-05-11T10:58:56.075192Z","shell.execute_reply":"2024-05-11T10:58:56.074234Z","shell.execute_reply.started":"2024-05-11T10:56:21.346828Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1: 100%|██████████| 543/543 [00:47<00:00, 11.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.408\n","Training Accuracy: 0.860\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:03<00:00, 35.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.291\n","Validation Accuracy: 0.899\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2: 100%|██████████| 543/543 [00:47<00:00, 11.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.249\n","Training Accuracy: 0.913\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:03<00:00, 35.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.255\n","Validation Accuracy: 0.913\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3: 100%|██████████| 543/543 [00:47<00:00, 11.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training Loss: 0.221\n","Training Accuracy: 0.923\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 117/117 [00:03<00:00, 36.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.245\n","Validation Accuracy: 0.915\n"]},{"name":"stderr","output_type":"stream","text":["Final Test Evaluation: 100%|██████████| 117/117 [00:03<00:00, 36.93it/s]"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.233\n","Validation Accuracy: 0.914\n","                    precision    recall  f1-score   support\n","\n","       Hate Speech       0.51      0.23      0.32       207\n","Offensive Language       0.93      0.96      0.95      2880\n","           Neither       0.88      0.91      0.89       631\n","\n","          accuracy                           0.91      3718\n","         macro avg       0.77      0.70      0.72      3718\n","      weighted avg       0.90      0.91      0.90      3718\n","\n","Test Accuracy: 0.913\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Main training loop\n","for epoch in range(1, 4):\n","    train(epoch)\n","    evaluate(val_loader)\n","\n","# Final evaluation on test set\n","labels, predictions = evaluate(test_loader, \"Final Test Evaluation\")\n","print(classification_report(labels, predictions, target_names=['Hate Speech', 'Offensive Language', 'Neither']))\n","\n","# Accuracy\n","accuracy = accuracy_score(labels, predictions)\n","print(f\"Test Accuracy: {accuracy:.3f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4985191,"sourceId":8382690,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
