{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74586,"databundleVersionId":8130765,"sourceType":"competition"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom os.path import join\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nimport torch.optim as optim\nfrom torchvision.models import resnet50","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:05:30.139404Z","iopub.execute_input":"2024-04-16T11:05:30.140143Z","iopub.status.idle":"2024-04-16T11:05:36.704728Z","shell.execute_reply.started":"2024-04-16T11:05:30.140112Z","shell.execute_reply":"2024-04-16T11:05:36.703899Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class AgeDataset(torch.utils.data.Dataset):\n\n    def __init__(self, data_path, annot_path, train=True):\n        super(AgeDataset, self).__init__()\n\n        self.annot_path = annot_path\n        self.data_path = data_path\n        self.train = train\n\n        self.ann = pd.read_csv(annot_path)\n        self.files = self.ann['file_id']\n        if train:\n            self.ages = self.ann['age']\n        self.transform = self._transform(224)\n\n    @staticmethod    \n    def _convert_image_to_rgb(image):\n        return image.convert(\"RGB\")\n\n    def _transform(self, n_px):\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        return Compose([\n            Resize(n_px),\n            self._convert_image_to_rgb,\n            ToTensor(),\n            Normalize(mean, std),\n        ])\n\n    def read_img(self, file_name):\n        im_path = join(self.data_path,file_name)   \n        img = Image.open(im_path)\n        img = self.transform(img)\n        return img\n\n    def __getitem__(self, index):\n        file_name = self.files[index]\n        img = self.read_img(file_name)\n        if self.train:\n            age = self.ages[index]\n            return img, age\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.files)\n\n\ntrain_path = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/train'\ntrain_ann = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/train.csv'\ntrain_dataset = AgeDataset(train_path, train_ann, train=True)\n\n\ntest_path = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/test'\ntest_ann = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/submission.csv'\ntest_dataset = AgeDataset(test_path, test_ann, train=False)\n\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:05:44.115493Z","iopub.execute_input":"2024-04-16T11:05:44.116307Z","iopub.status.idle":"2024-04-16T11:05:44.166438Z","shell.execute_reply.started":"2024-04-16T11:05:44.116276Z","shell.execute_reply":"2024-04-16T11:05:44.165562Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class AgePredictionModel(nn.Module):\n    def __init__(self):\n        super(AgePredictionModel, self).__init__()\n        # Load a pre-trained ResNet50 model\n        self.resnet = resnet50(pretrained=True)\n        # Modify the last layer for age prediction\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1)  # Output age as a single value\n\n    def forward(self, x):\n        return self.resnet(x)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AgePredictionModel().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:05:49.302272Z","iopub.execute_input":"2024-04-16T11:05:49.303175Z","iopub.status.idle":"2024-04-16T11:05:51.006592Z","shell.execute_reply.started":"2024-04-16T11:05:49.303140Z","shell.execute_reply":"2024-04-16T11:05:51.005747Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 139MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.L1Loss()  # Mean Absolute Error (MAE) loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:05:57.415397Z","iopub.execute_input":"2024-04-16T11:05:57.415767Z","iopub.status.idle":"2024-04-16T11:05:57.422466Z","shell.execute_reply.started":"2024-04-16T11:05:57.415739Z","shell.execute_reply":"2024-04-16T11:05:57.421468Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels.float().unsqueeze(1))  # Labels need to be reshaped for L1 loss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:06:03.163201Z","iopub.execute_input":"2024-04-16T11:06:03.163797Z","iopub.status.idle":"2024-04-16T11:34:22.300549Z","shell.execute_reply.started":"2024-04-16T11:06:03.163765Z","shell.execute_reply":"2024-04-16T11:34:22.299292Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 8.6111\nEpoch [2/10], Loss: 6.6247\nEpoch [3/10], Loss: 6.1904\nEpoch [4/10], Loss: 5.8995\nEpoch [5/10], Loss: 5.5815\nEpoch [6/10], Loss: 5.3361\nEpoch [7/10], Loss: 5.0948\nEpoch [8/10], Loss: 4.8774\nEpoch [9/10], Loss: 4.7537\nEpoch [10/10], Loss: 4.4949\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(loader, model):\n    model.eval()\n    predictions = []\n\n    for img in tqdm(loader):\n        img = img.to(device)\n\n        pred = model(img)\n        predictions.extend(pred.flatten().detach().tolist())\n\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:34:37.490132Z","iopub.execute_input":"2024-04-16T11:34:37.490822Z","iopub.status.idle":"2024-04-16T11:34:37.499732Z","shell.execute_reply.started":"2024-04-16T11:34:37.490791Z","shell.execute_reply":"2024-04-16T11:34:37.498701Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Evaluation on test set\nmodel.eval()\ntest_predictions = predict(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:34:40.754116Z","iopub.execute_input":"2024-04-16T11:34:40.754469Z","iopub.status.idle":"2024-04-16T11:34:58.482169Z","shell.execute_reply.started":"2024-04-16T11:34:40.754445Z","shell.execute_reply":"2024-04-16T11:34:58.481213Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 31/31 [00:17<00:00,  1.75it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save predictions to a CSV file\nsubmit = pd.read_csv('/kaggle/input/smai-24-age-prediction/content/faces_dataset/submission.csv')\nsubmit['age'] = test_predictions\nsubmit.head()\nsubmit.to_csv('baseline.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:35:01.591525Z","iopub.execute_input":"2024-04-16T11:35:01.592288Z","iopub.status.idle":"2024-04-16T11:35:01.612165Z","shell.execute_reply.started":"2024-04-16T11:35:01.592259Z","shell.execute_reply":"2024-04-16T11:35:01.611355Z"},"trusted":true},"execution_count":9,"outputs":[]}]}